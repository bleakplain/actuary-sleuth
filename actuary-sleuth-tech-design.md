# Actuary Sleuth Skill - å®Œæ•´æŠ€æœ¯æ–¹æ¡ˆ v3.0

**ç‰ˆæœ¬**: v3.0
**æ—¥æœŸ**: 2026-02-15
**æ¶æ„**: SKILL.md å·¥ä½œæµç¼–æ’ + Python è„šæœ¬å®ç°

---

## ä¸€ã€ç³»ç»Ÿæ¦‚è¿°

### 1.1 é¡¹ç›®å®šä½

Actuary Sleuth æ˜¯ä¸€ä¸ªåŸºäº SKILL.md å·¥ä½œæµç¼–æ’è§„èŒƒçš„ç²¾ç®—å®¡æ ¸ç³»ç»Ÿï¼Œé€šè¿‡é£ä¹¦ Channel ä¸ç”¨æˆ·äº¤äº’ï¼Œå®ç°ä¿é™©äº§å“æ–‡æ¡£çš„è‡ªåŠ¨åŒ–å®¡æ ¸ã€‚

### 1.2 æ ¸å¿ƒèƒ½åŠ›

| åŠŸèƒ½ | æè¿° |
|------|------|
| æ–‡æ¡£å®¡æ ¸ | å®¡æ ¸ä¿é™©äº§å“æ¡æ¬¾ï¼Œæ£€æŸ¥è´Ÿé¢æ¸…å•å’Œæ³•è§„åˆè§„æ€§ |
| æ³•è§„æŸ¥è¯¢ | æ”¯æŒæ¡æ¬¾ç¼–å·ç²¾ç¡®æŸ¥è¯¢å’Œè¯­ä¹‰æ£€ç´¢ |
| æŠ¥å‘Šç”Ÿæˆ | è‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–å®¡æ ¸æŠ¥å‘Šå¹¶æ¨é€åˆ°é£ä¹¦ |

### 1.3 æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯ |
|------|------|
| å·¥ä½œæµç¼–æ’ | SKILL.md è§„èŒƒ |
| ä¸šåŠ¡é€»è¾‘ | Python 3.10+ |
| å‘é‡æ£€ç´¢ | LanceDB + Ollama embeddings |
| ç»“æ„åŒ–å­˜å‚¨ | SQLite |
| LLM | Ollama (qwen2:7b) |
| æ–‡æ¡£è½¬æ¢ | feishu2md |
| OCR | PaddleOCR |

---

## äºŒã€ç³»ç»Ÿæ¶æ„

### 2.1 æ•´ä½“æ¶æ„å›¾

```mermaid
flowchart TB
    subgraph UI["ç”¨æˆ·äº¤äº’å±‚"]
        FEISHU["ğŸ“± é£ä¹¦å®¢æˆ·ç«¯<br/>â”â”â”â”â”â”â”â”â”â”â”â”<br/>â€¢ ç§èŠ/ç¾¤èŠ<br/>â€¢ å‘é€æ–‡æ¡£é“¾æ¥ + @æœºå™¨äºº + "å¼€å§‹å®¡æ ¸""]
    end

    subgraph GATEWAY["OpenClaw ç½‘å…³å±‚"]
        CHANNEL["Feishu Channel<br/>â”â”â”â”â”â”â”â”â”â”â”â”<br/>â€¢ Webhook æ¥æ”¶é£ä¹¦äº‹ä»¶<br/>â€¢ æ¶ˆæ¯è§£æå’ŒæŒ‡ä»¤è¯†åˆ«<br/>â€¢ å“åº”æ ¼å¼åŒ–å’Œæ¨é€<br/>â€¢ è§£æ SKILL.mdï¼Œè°ƒç”¨è„šæœ¬"]
    end

    subgraph SKILL["Actuary Sleuth Skill"]
        SKILL_MD["SKILL.md<br/>â”â”â”â”â”â”â”â”â”â”â”â”<br/>â€¢ å·¥ä½œæµç¼–æ’å®šä¹‰<br/>â€¢ å·¥å…·/å‚æ•°/è¾“å‡ºå£°æ˜<br/>â€¢ æ‰§è¡Œæµç¨‹è¯´æ˜"]
        SCRIPTS["Python è„šæœ¬å±‚<br/>â”â”â”â”â”â”â”â”â”â”â”â”<br/>â€¢ æ–‡æ¡£é¢„å¤„ç†<br/>â€¢ å®¡æ ¸åˆ†æ<br/>â€¢ æ³•è§„æŸ¥è¯¢<br/>â€¢ æŠ¥å‘Šç”Ÿæˆ"]
        SKILL_MD --> SCRIPTS
    end

    subgraph STORAGE["æ•°æ®å­˜å‚¨å±‚"]
        SQLITE["SQLite<br/>æ³•è§„åº“"]
        LANCE["LanceDB<br/>å‘é‡åº“"]
        CACHE["æ–‡ä»¶ç¼“å­˜"]
        CONFIG["é…ç½®æ–‡ä»¶"]
        OLLAMA["Ollama<br/>LLM æœåŠ¡"]
    end

    FEISHU --> CHANNEL
    CHANNEL --> SKILL_MD
    SCRIPTS --> SQLITE
    SCRIPTS --> LANCE
    SCRIPTS --> CACHE
    SCRIPTS --> CONFIG
    SCRIPTS --> OLLAMA

    style UI fill:#e1f5ff
    style GATEWAY fill:#fff3e0
    style SKILL fill:#f3e5f5
    style STORAGE fill:#e8f5e9
    style FEISHU fill:#4fc3f7
    style CHANNEL fill:#ffb74d
    style SKILL_MD fill:#ba68c8
    style SCRIPTS fill:#9575cd
    style SQLITE fill:#81c784
    style LANCE fill:#81c784
    style CACHE fill:#81c784
    style CONFIG fill:#81c784
    style OLLAMA fill:#ff8a65
```

### 2.2 èŒè´£è¾¹ç•Œ

| å±‚çº§ | èŒè´£ |
|------|------|
| **Feishu Channel** | æ¶ˆæ¯æ”¶å‘ã€æŒ‡ä»¤è§£æã€å“åº”æ ¼å¼åŒ–ã€SKILL.md è§£æå’Œè„šæœ¬è°ƒç”¨ |
| **SKILL.md** | å·¥ä½œæµç¼–æ’ã€å·¥å…·å®šä¹‰ã€æ‰§è¡Œå£°æ˜ |
| **Python è„šæœ¬** | æ–‡æ¡£å¤„ç†ã€å®¡æ ¸åˆ†æã€æŠ¥å‘Šç”Ÿæˆ |

---

## ä¸‰ã€ç›®å½•ç»“æ„

```
/root/.openclaw/workspace/skills/actuary-sleuth/
â”œâ”€â”€ SKILL.md                 # å·¥ä½œæµç¼–æ’è§„èŒƒï¼ˆæ ¸å¿ƒï¼‰
â”œâ”€â”€ skill.json              # Skill é…ç½®æ¸…å•
â”œâ”€â”€ scripts/               # Python è„šæœ¬ï¼ˆåŠŸèƒ½å®ç°ï¼‰
â”‚   â”œâ”€â”€ template.py       # è„šæœ¬æ¨¡æ¿ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
â”‚   â”œâ”€â”€ preprocess.py     # æ–‡æ¡£é¢„å¤„ç†
â”‚   â”œâ”€â”€ audit.py         # å®¡æ ¸å¼•æ“
â”‚   â”œâ”€â”€ query.py         # æ³•è§„æŸ¥è¯¢
â”‚   â”œâ”€â”€ check.py         # è´Ÿé¢æ¸…å•æ£€æŸ¥
â”‚   â”œâ”€â”€ report.py        # æŠ¥å‘Šç”Ÿæˆ
â”‚   â”œâ”€â”€ scoring.py       # è¯„åˆ†æ¨¡å—
â”‚   â”œâ”€â”€ lib/             # Python åº“
â”‚   â”‚   â”œâ”€â”€ db.py        # æ•°æ®åº“æ“ä½œ
â”‚   â”‚   â”œâ”€â”€ vector_store.py # å‘é‡æ£€ç´¢
â”‚   â”‚   â”œâ”€â”€ ollama.py     # LLM è°ƒç”¨
â”‚   â”‚   â””â”€â”€ feishu2md.py # æ–‡æ¡£è½¬æ¢
â”‚   â”œâ”€â”€ init_db.py       # åˆå§‹åŒ–æ•°æ®åº“
â”‚   â”œâ”€â”€ import_regs.py   # å¯¼å…¥æ³•è§„æ•°æ®
â”‚   â”œâ”€â”€ build_vectors.py # æ„å»ºå‘é‡ç´¢å¼•
â”‚   â”œâ”€â”€ config/          # é…ç½®æ–‡ä»¶
â”‚   â”‚   â””â”€â”€ settings.json
â”‚   â””â”€â”€ requirements.txt # Python ä¾èµ–
â”‚
â”œâ”€â”€ data/                # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ actuary.db      # SQLite æ•°æ®åº“
â”‚   â””â”€â”€ lancedb/        # LanceDB å‘é‡åº“
â”‚
â””â”€â”€ references/          # æ³•è§„çŸ¥è¯†åº“
    â”œâ”€â”€ 01_ä¿é™©æ³•ç›¸å…³ç›‘ç®¡è§„å®š.md
    â”œâ”€â”€ 02_è´Ÿé¢æ¸…å•.md
    â””â”€â”€ ...
```

---

## å››ã€å·¥ä½œæµç¼–æ’ä¸é…ç½®

### 4.1 SKILL.md ç»“æ„

SKILL.md æ˜¯å·¥ä½œæµç¼–æ’è§„èŒƒæ–‡ä»¶ï¼Œå®šä¹‰äº†æŠ€èƒ½çš„å·¥ä½œæµç¨‹ã€å·¥å…·æ¥å£å’Œæ‰§è¡Œå£°æ˜ã€‚

#### 4.1.1 åŸºæœ¬å…ƒæ•°æ®

```yaml
---
name: actuary-sleuth
description: Use when reviewing insurance product clauses for compliance, checking against regulatory negative lists, calculating pricing reasonableness, or querying insurance regulations and laws. Use forç²¾ç®—å¸ˆæ—¥å¸¸è¯„å®¡å·¥ä½œ includingæ–°äº§å“æ¡æ¬¾å®¡æ ¸ã€æ³•è§„æŸ¥è¯¢ã€è´Ÿé¢æ¸…å•æ£€æŸ¥ã€å®šä»·åˆç†æ€§è®¡ç®—å’Œè¯„å®¡æŠ¥å‘Šç”Ÿæˆ.
---
```

#### 4.1.2 å·¥å…·å®šä¹‰ï¼ˆToolsï¼‰

| å·¥å…·å | åŠŸèƒ½ | æ‰§è¡Œè„šæœ¬ |
|--------|------|----------|
| `audit_document` | å®¡æ ¸ä¿é™©äº§å“æ–‡æ¡£ | `scripts/audit.py` |
| `query_regulation` | æŸ¥è¯¢ä¿é™©æ³•è§„ | `scripts/query.py` |
| `check_negative_list` | æ£€æŸ¥è´Ÿé¢æ¸…å• | `scripts/check.py` |

**audit_document å·¥å…·**ï¼š
- è¾“å…¥ï¼šæ–‡æ¡£å†…å®¹(Markdown)ã€æ–‡æ¡£URLã€å®¡æ ¸ç±»å‹
- è¾“å‡ºï¼šå®¡æ ¸æŠ¥å‘Š(JSON)
- æµç¨‹ï¼šé¢„å¤„ç† â†’ è´Ÿé¢æ¸…å•æ£€æŸ¥ â†’ æ³•è§„æ£€ç´¢ â†’ æŠ¥å‘Šç”Ÿæˆ

**query_regulation å·¥å…·**ï¼š
- è¾“å…¥ï¼šæŸ¥è¯¢è¯ã€æœç´¢ç±»å‹
- è¾“å‡ºï¼šæ³•è§„å†…å®¹åˆ—è¡¨(JSON)
- æ”¯æŒç²¾ç¡®æŸ¥è¯¢ã€è¯­ä¹‰æ£€ç´¢ã€æ··åˆæœç´¢

**check_negative_list å·¥å…·**ï¼š
- è¾“å…¥ï¼šäº§å“æ¡æ¬¾æ•°ç»„
- è¾“å‡ºï¼šè¿è§„ç‚¹åˆ—è¡¨(JSON)
- åŒ…å«è¿è§„æè¿°ã€ä¸¥é‡ç¨‹åº¦ã€æ•´æ”¹å»ºè®®

#### 4.1.3 é…ç½®å‚æ•°ï¼ˆConfigurationï¼‰

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `scriptsPath` | Python è„šæœ¬è·¯å¾„ | ./scripts |
| `dataPath` | æ•°æ®ç›®å½•è·¯å¾„ | ./data |
| `pythonEnv` | Python ç¯å¢ƒ | python3 |

#### 4.1.4 ä¾èµ–è¦æ±‚ï¼ˆRequirementsï¼‰

- ç½‘ç»œæƒé™ï¼šfeishu
- æ–‡ä»¶æƒé™ï¼šread, write
- ä¾èµ–ï¼špython3, sqlite3, lancedb, ollama

### 4.2 skill.json é…ç½®

```json
{
  "id": "actuary-sleuth",
  "name": "Actuary Sleuth",
  "version": "3.0.0",
  "readme": "SKILL.md",
  "config": {
    "scriptsPath": "./scripts",
    "dataPath": "./data",
    "pythonEnv": "python3",
    "lancedbUri": "./data/lancedb",
    "ollamaHost": "http://localhost:11434",
    "ollamaModel": "qwen2:7b",
    "ollamaEmbedModel": "nomic-embed-text"
  }
}
```

### 4.3 çŸ¥è¯†åº“

æœ¬æŠ€èƒ½å†…ç½®å®Œæ•´çš„ç²¾ç®—å®¡æ ¸æ³•è§„çŸ¥è¯†åº“ï¼š

#### åŸºç¡€æ³•è§„ï¼ˆP0ï¼‰
- `01_ä¿é™©æ³•ç›¸å…³ç›‘ç®¡è§„å®š.md` - ä¿é™©æ³•æ ¸å¿ƒæ¡æ¬¾
- `02_è´Ÿé¢æ¸…å•.md` - 22ä¸ªè¿è§„ç‚¹è¯¦ç»†è¯´æ˜
- `03_æ¡æ¬¾è´¹ç‡ç®¡ç†åŠæ³•.md` - è´¹ç”¨ç‡ç›‘ç®¡è§„å®š
- `04_ä¿¡æ¯æŠ«éœ²è§„åˆ™.md` - ä¿¡æ¯æŠ«éœ²è¦æ±‚

#### äº§å“å¼€å‘è§„èŒƒï¼ˆP0ï¼‰
- `05_å¥åº·ä¿é™©äº§å“å¼€å‘.md` - å¥åº·é™©å¼€å‘è§„èŒƒ
- `06_æ™®é€šå‹äººèº«ä¿é™©.md` - æ™®é€šå‹äº§å“è§„å®š
- `07_åˆ†çº¢å‹äººèº«ä¿é™©.md` - åˆ†çº¢å‹äº§å“è§„å®š
- `08_çŸ­æœŸå¥åº·ä¿é™©.md` - çŸ­æœŸå¥åº·é™©è§„å®š
- `09_æ„å¤–ä¼¤å®³ä¿é™©.md` - æ„å¤–é™©è§„å®š
- `10_äº’è”ç½‘ä¿é™©äº§å“.md` - äº’è”ç½‘äº§å“è§„èŒƒ
- `11_ç¨ä¼˜å¥åº·é™©.md` - ç¨ä¼˜å¥åº·é™©è§„å®š
- `12_ä¸‡èƒ½å‹äººèº«ä¿é™©.md` - ä¸‡èƒ½é™©è§„å®š
- `13_å…¶ä»–é™©ç§äº§å“.md` - å…¶ä»–é™©ç§è§„å®š
- `14_ç»¼åˆç›‘ç®¡è§„å®š.md` - ç»¼åˆç›‘ç®¡è¦æ±‚

### 4.4 ä½¿ç”¨åœºæ™¯

| åœºæ™¯ | è¾“å…¥ | è¾“å‡º | ä¼˜å…ˆçº§ |
|------|------|------|--------|
| äº§å“æ–‡æ¡£å®¡æ ¸ | Wordæ–‡æ¡£ | ç»“æ„åŒ–äº§å“æ•°æ® + è¿è§„æ£€æŸ¥ç»“æœ | P0 |
| è´Ÿé¢æ¸…å•æ£€æŸ¥ | äº§å“æ¡æ¬¾ | 22ä¸ªè¿è§„ç‚¹æ£€æŸ¥ç»“æœ + æ•´æ”¹å»ºè®® | P0 |
| æ³•è§„å¿«é€ŸæŸ¥è¯¢ | æ¡æ¬¾ç¼–å·/å…³é”®è¯ | å®Œæ•´æ¡æ¬¾å†…å®¹ + æ ‡å‡†å¼•ç”¨æ ¼å¼ | P0 |
| å®šä»·åˆç†æ€§è®¡ç®— | å®šä»·å‚æ•° | åå·®åˆ†æ + åˆç†æ€§åˆ¤æ–­ | P0 |
| è¯„å®¡æŠ¥å‘Šç”Ÿæˆ | å®¡æ ¸ç»“æœ | Word/PDFæ ¼å¼æŠ¥å‘Š | P0 |
| æ™ºèƒ½æ£€ç´¢ | è‡ªç„¶è¯­è¨€æè¿° | ç›¸å…³æ³•è§„æ¡æ¬¾ | P1 |

---

## äº”ã€Python è„šæœ¬å®ç°

### 5.1 ç»Ÿä¸€æ¥å£æ¨¡æ¿

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Actuary Sleuth Script Template
ç»Ÿä¸€è„šæœ¬æ¥å£è§„èŒƒ
"""
import argparse
import json
import sys
from pathlib import Path

# æ·»åŠ  lib ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / 'lib'))

def main():
    parser = argparse.ArgumentParser(description='Actuary Sleuth Script')
    parser.add_argument('--input', required=True, help='JSON input file')
    parser.add_argument('--config', default='./config/settings.json', help='Config file')
    args = parser.parse_args()

    # è¯»å–è¾“å…¥
    with open(args.input, 'r', encoding='utf-8') as f:
        params = json.load(f)

    # æ‰§è¡Œä¸šåŠ¡é€»è¾‘
    try:
        result = execute(params)
        # è¾“å‡ºç»“æœï¼ˆJSONæ ¼å¼ï¼‰
        print(json.dumps(result, ensure_ascii=False, indent=2))
        return 0
    except Exception as e:
        # é”™è¯¯è¾“å‡º
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__
        }
        print(json.dumps(error_result, ensure_ascii=False), file=sys.stderr)
        return 1

def execute(params):
    """å…·ä½“ä¸šåŠ¡é€»è¾‘å®ç° - å­ç±»å¿…é¡»è¦†ç›–"""
    raise NotImplementedError("Subclasses must implement execute()")

if __name__ == '__main__':
    sys.exit(main())
```

### 5.2 audit.pyï¼ˆå®¡æ ¸å¼•æ“ï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®¡æ ¸å¼•æ“ - ä¸»å…¥å£
"""
from template import main
from lib import preprocess, check, query, scoring, report

def execute(params):
    """æ‰§è¡Œå®Œæ•´å®¡æ ¸æµç¨‹"""
    # 1. æ–‡æ¡£é¢„å¤„ç†
    doc = preprocess.process(params['documentContent'])

    # 2. è´Ÿé¢æ¸…å•æ£€æŸ¥
    violations = check.negative_list(doc['clauses'])

    # 3. æ³•è§„åˆè§„æ£€æŸ¥
    audit_type = params.get('auditType', 'full')
    if audit_type != 'negative-only':
        for v in violations:
            v['regulations'] = query.search_regulations(v['description'])

    # 4. å®šä»·åˆ†æ
    pricing = None
    if 'pricing_data' in doc and audit_type == 'full':
        pricing = scoring.analyze_pricing(doc['pricing_data'])

    # 5. è®¡ç®—ç»¼åˆè¯„åˆ†
    score = scoring.calculate_score(violations, pricing)

    # 6. ç”ŸæˆæŠ¥å‘Š
    return report.generate({
        'violations': violations,
        'pricing': pricing,
        'score': score,
        'document': doc,
        'metadata': {
            'audit_type': audit_type,
            'document_url': params.get('documentUrl', ''),
            'timestamp': datetime.now().isoformat()
        }
    })

if __name__ == '__main__':
    from datetime import datetime
    main()
```

### 5.3 query.pyï¼ˆæ³•è§„æŸ¥è¯¢ï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ³•è§„æŸ¥è¯¢è„šæœ¬
"""
from template import main
from lib import db, vector_store, ollama

def execute(params):
    """æ‰§è¡Œæ³•è§„æŸ¥è¯¢"""
    query_text = params['query']
    search_type = params.get('searchType', 'hybrid')

    results = []

    # ç²¾ç¡®æŸ¥è¯¢
    if search_type in ['exact', 'hybrid']:
        exact = db.find_regulation(query_text)
        if exact:
            results.append({
                'type': 'exact',
                'content': exact['content'],
                'law_name': exact['law_name'],
                'article_number': exact['article_number'],
                'category': exact['category'],
                'score': 1.0
            })

    # è¯­ä¹‰æ£€ç´¢
    if search_type in ['semantic', 'hybrid']:
        query_vec = ollama.embed(query_text)
        semantic = lancedb.search(query_vec, top_k=5)
        for item in semantic:
            results.append({
                'type': 'semantic',
                'content': item['content'],
                'law_name': item['metadata']['law_name'],
                'article_number': item['metadata']['article_number'],
                'score': item['score']
            })

    # æ’åºè¿”å›
    results.sort(key=lambda x: x['score'], reverse=True)

    return {
        'success': True,
        'query': query_text,
        'search_type': search_type,
        'results': results[:5],
        'count': len(results[:5])
    }

if __name__ == '__main__':
    main()
```

### 5.4 check.pyï¼ˆè´Ÿé¢æ¸…å•æ£€æŸ¥ï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è´Ÿé¢æ¸…å•æ£€æŸ¥è„šæœ¬
"""
from template import main
from lib import db

def execute(params):
    """æ‰§è¡Œè´Ÿé¢æ¸…å•æ£€æŸ¥"""
    clauses = params['clauses']

    # è·å–è´Ÿé¢æ¸…å•è§„åˆ™
    rules = db.get_negative_list()

    # æ‰§è¡Œæ£€æŸ¥
    violations = []
    for idx, clause in enumerate(clauses):
        for rule in rules:
            if match_rule(clause, rule):
                violations.append({
                    'clause_index': idx,
                    'clause_text': clause[:100] + '...' if len(clause) > 100 else clause,
                    'rule': rule['rule_number'],
                    'description': rule['description'],
                    'severity': rule['severity'],
                    'category': rule['category'],
                    'remediation': rule['remediation']
                })

    return {
        'success': True,
        'violations': violations,
        'count': len(violations),
        'summary': group_by_severity(violations)
    }

def match_rule(clause, rule):
    """è§„åˆ™åŒ¹é…é€»è¾‘"""
    # å…³é”®è¯åŒ¹é…
    keywords = rule.get('keywords', [])
    for keyword in keywords:
        if keyword in clause:
            return True

    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
    import re
    patterns = rule.get('patterns', [])
    for pattern in patterns:
        if re.search(pattern, clause):
            return True

    return False

def group_by_severity(violations):
    """æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„"""
    summary = {
        'high': sum(1 for v in violations if v['severity'] == 'high'),
        'medium': sum(1 for v in violations if v['severity'] == 'medium'),
        'low': sum(1 for v in violations if v['severity'] == 'low')
    }
    return summary

if __name__ == '__main__':
    main()
```

---

## å…­ã€Python åº“æ¨¡å—

### 6.1 lib/db.pyï¼ˆæ•°æ®åº“æ“ä½œï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“æ“ä½œæ¨¡å—
"""
import sqlite3
import json
from pathlib import Path

DB_PATH = Path(__file__).parent.parent.parent / 'data' / 'actuary.db'

def get_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn

def find_regulation(article_number):
    """ç²¾ç¡®æŸ¥æ‰¾æ³•è§„æ¡æ¬¾"""
    conn = get_connection()
    cur = conn.cursor()

    cur.execute('''
        SELECT * FROM regulations
        WHERE article_number = ?
    ''', (article_number,))

    row = cur.fetchone()
    conn.close()

    if row:
        return dict(row)
    return None

def search_regulations(keyword):
    """å…³é”®è¯æœç´¢æ³•è§„"""
    conn = get_connection()
    cur = conn.cursor()

    cur.execute('''
        SELECT * FROM regulations
        WHERE content LIKE ? OR article_number LIKE ?
        LIMIT 20
    ''', (f'%{keyword}%', f'%{keyword}%'))

    rows = cur.fetchall()
    conn.close()

    return [dict(row) for row in rows]

def get_negative_list():
    """è·å–è´Ÿé¢æ¸…å•"""
    conn = get_connection()
    cur = conn.cursor()

    cur.execute('SELECT * FROM negative_list ORDER BY severity DESC')
    rows = cur.fetchall()
    conn.close()

    return [dict(row) for row in rows]

def save_audit_record(record):
    """ä¿å­˜å®¡æ ¸è®°å½•"""
    conn = get_connection()
    cur = conn.cursor()

    cur.execute('''
        INSERT INTO audit_history (id, user_id, document_url, violations, score)
        VALUES (?, ?, ?, ?, ?)
    ''', (
        record['id'],
        record.get('user_id', ''),
        record.get('document_url', ''),
        json.dumps(record.get('violations', []), ensure_ascii=False),
        record.get('score', 0)
    ))

    conn.commit()
    conn.close()
```

### 6.2 lib/vector_store.pyï¼ˆå‘é‡æ£€ç´¢ï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘é‡æ£€ç´¢æ¨¡å—
"""
import lancedb
from pathlib import Path

DB_URI = str(Path(__file__).parent.parent.parent / 'data' / 'lancedb')

class VectorDB:
    _instance = None
    _tables = {}

    @classmethod
    def connect(cls):
        """è¿æ¥ LanceDB"""
        if cls._instance is None:
            cls._instance = lancedb.connect(DB_URI)
        return cls._instance

    @classmethod
    def get_table(cls, table_name='regulations_vectors'):
        """è·å–è¡¨"""
        if table_name not in cls._tables:
            db = cls.connect()
            try:
                cls._tables[table_name] = db.open_table(table_name)
            except:
                # è¡¨ä¸å­˜åœ¨ï¼Œè¿”å› None
                return None
        return cls._tables[table_name]

    @classmethod
    def search(cls, query_vector, top_k=5, table_name='regulations_vectors'):
        """å‘é‡æœç´¢"""
        table = cls.get_table(table_name)
        if table is None:
            return []

        results = table.vectorSearch(query_vector).limit(top_k).to_pydict()

        return [
            {
                'content': r['chunk_text'],
                'metadata': r['metadata'],
                'score': 1 / (1 + r.get('_distance', 0))
            }
            for r in results
        ]

    @classmethod
    def add_vectors(cls, data, table_name='regulations_vectors'):
        """æ·»åŠ å‘é‡"""
        db = cls.connect()

        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        existing_tables = db.table_names()
        if table_name not in existing_tables:
            # åˆ›å»ºæ–°è¡¨
            import pyarrow as pa
            schema = pa.schema([
                pa.field('id', pa.string()),
                pa.field('regulation_id', pa.string()),
                pa.field('chunk_text', pa.string()),
                pa.field('vector', pa.list_(pa.float32())),
                pa.field('metadata', pa.string())
            ])
            table = db.create_table(table_name, schema=schema)
        else:
            table = db.open_table(table_name)

        table.add(data)
        cls._tables[table_name] = table
```

### 6.3 lib/ollama.pyï¼ˆLLM è°ƒç”¨ï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM è°ƒç”¨æ¨¡å—
"""
import requests
import json

OLLAMA_HOST = 'http://localhost:11434'
EMBED_MODEL = 'nomic-embed-text'
CHAT_MODEL = 'qwen2:7b'

def embed(text):
    """ç”Ÿæˆæ–‡æœ¬å‘é‡"""
    try:
        response = requests.post(
            f'{OLLAMA_HOST}/api/embeddings',
            json={
                'model': EMBED_MODEL,
                'prompt': text
            },
            timeout=30
        )
        response.raise_for_status()
        return response.json()['embedding']
    except Exception as e:
        raise Exception(f"Ollama embed error: {str(e)}")

def generate(prompt, system=None):
    """ç”Ÿæˆæ–‡æœ¬"""
    data = {
        'model': CHAT_MODEL,
        'prompt': prompt,
        'stream': False
    }

    if system:
        data['system'] = system

    try:
        response = requests.post(
            f'{OLLAMA_HOST}/api/generate',
            json=data,
            timeout=120
        )
        response.raise_for_status()
        return response.json()['response']
    except Exception as e:
        raise Exception(f"Ollama generate error: {str(e)}")

def analyze_compliance(clause, regulations):
    """åˆ†ææ¡æ¬¾åˆè§„æ€§"""
    prompt = f"""ä½œä¸ºä¿é™©ç²¾ç®—ä¸“å®¶ï¼Œè¯·åˆ¤æ–­ä»¥ä¸‹æ¡æ¬¾æ˜¯å¦è¿è§„ï¼š

ã€æ¡æ¬¾å†…å®¹ã€‘
{clause}

ã€ç›¸å…³æ³•è§„ã€‘
{chr(10).join(regulations[:3])}

è¯·è¿”å›JSONæ ¼å¼ï¼ˆä»…è¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼‰ï¼š
{{
    "is_violation": trueæˆ–false,
    "reason": "è¿è§„åŸå› æˆ–åˆè§„è¯´æ˜",
    "severity": "highæˆ–mediumæˆ–low",
    "suggestion": "æ•´æ”¹å»ºè®®"
}}"""

    try:
        result = generate(prompt)
        # å°è¯•è§£æ JSON
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        result = result.strip()
        if result.startswith('```'):
            result = result.split('\n', 1)[1]
        if result.endswith('```'):
            result = result.rsplit('\n', 1)[0]
        if result.startswith('json'):
            result = result[4:]

        return json.loads(result)
    except:
        # è§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ
        return {
            "is_violation": False,
            "reason": "æ— æ³•è§£æ",
            "severity": "low",
            "suggestion": "è¯·äººå·¥å¤æ ¸"
        }
```

### 6.4 lib/feishu2md.pyï¼ˆæ–‡æ¡£è½¬æ¢ï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é£ä¹¦æ–‡æ¡£è½¬æ¢æ¨¡å—
"""
import subprocess
import tempfile
from pathlib import Path

def convert_feishu_to_markdown(document_url):
    """å°†é£ä¹¦æ–‡æ¡£è½¬æ¢ä¸º Markdown"""
    try:
        # è°ƒç”¨ feishu2md å·¥å…·
        result = subprocess.run(
            ['feishu2md', document_url],
            capture_output=True,
            text=True,
            timeout=60
        )

        if result.returncode == 0:
            return result.stdout
        else:
            raise Exception(f"feishu2md error: {result.stderr}")

    except FileNotFoundError:
        # feishu2md æœªå®‰è£…ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
        return convert_fallback(document_url)

def convert_fallback(document_url):
    """å¤‡ç”¨è½¬æ¢æ–¹æ¡ˆ"""
    # è¿™é‡Œå¯ä»¥å®ç°ä¸€ä¸ªç®€å•çš„è½¬æ¢é€»è¾‘
    # æˆ–è€…æç¤ºç”¨æˆ·æ‰‹åŠ¨è½¬æ¢
    raise NotImplementedError(
        "è¯·å®‰è£… feishu2md å·¥å…·æˆ–æ‰‹åŠ¨å°†æ–‡æ¡£è½¬æ¢ä¸º Markdown æ ¼å¼"
    )
```

---

## ä¸ƒã€æ•°æ®æ¨¡å‹

### 7.1 SQLite è¡¨ç»“æ„

```sql
-- æ³•è§„åº“è¡¨
CREATE TABLE IF NOT EXISTS regulations (
    id TEXT PRIMARY KEY,
    law_name TEXT NOT NULL,
    article_number TEXT,
    content TEXT NOT NULL,
    category TEXT,
    tags TEXT,
    effective_date TEXT,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP
);
CREATE INDEX IF NOT EXISTS idx_article ON regulations(law_name, article_number);

-- è´Ÿé¢æ¸…å•è¡¨
CREATE TABLE IF NOT EXISTS negative_list (
    id INTEGER PRIMARY KEY,
    rule_number TEXT UNIQUE,
    description TEXT NOT NULL,
    severity TEXT,
    category TEXT,
    remediation TEXT,
    keywords TEXT,
    patterns TEXT,
    version TEXT,
    effective_date TEXT
);

-- å®¡æ ¸å†å²è¡¨
CREATE TABLE IF NOT EXISTS audit_history (
    id TEXT PRIMARY KEY,
    user_id TEXT,
    document_url TEXT,
    document_type TEXT,
    violations TEXT,
    score REAL,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP
);
```

### 7.2 LanceDB è¡¨ç»“æ„

```python
# æ³•è§„å‘é‡è¡¨
regulations_vectors = {
    'id': str,              # å”¯ä¸€æ ‡è¯†
    'regulation_id': str,   # å…³è”æ³•è§„ID
    'chunk_text': str,      # æ–‡æœ¬åˆ†å—
    'vector': list(float),  # 768ç»´å‘é‡
    'metadata': str         # å…ƒæ•°æ®ï¼ˆJSONå­—ç¬¦ä¸²ï¼‰
}
```

---

## å…«ã€æ•°æ®å¤„ç†æµç¨‹

### 8.1 å®Œæ•´æ•°æ®æµå›¾

```mermaid
flowchart TD
    subgraph INPUT["ç”¨æˆ·è¾“å…¥"]
        DOC_IN["æ–‡æ¡£å†…å®¹<br/>documentContent"]
        URL_IN["æ–‡æ¡£URL<br/>documentUrl"]
        TYPE_IN["å®¡æ ¸ç±»å‹<br/>auditType"]
    end

    subgraph STEP1["Step 1: æ–‡æ¡£é¢„å¤„ç†<br/>preprocess.py"]
        PARSE1["parse_document()<br/>è§£ææ–‡æ¡£ç»“æ„"]
        EXT_PROD["extract_product_info()<br/>æå–äº§å“ä¿¡æ¯"]
        EXT_CLAUSE["extract_clauses()<br/>æå–æ¡æ¬¾åˆ—è¡¨"]
        EXT_PRICE["extract_pricing_params()<br/>æå–å®šä»·å‚æ•°"]
    end

    subgraph DATA1["ä¸­é—´æ•°æ® 1"]
        PROD_INFO["product_info<br/>{product_name, company,<br/>type, period, ...}"]
        CLAUSES["clauses<br/>['ç¬¬1æ¡...', 'ç¬¬2æ¡...', ...]"]
        PRICE_PARAMS["pricing_params<br/>{mortality_rate,<br/>interest_rate,<br/>expense_rate, ...}"]
    end

    subgraph STEP2["Step 2: è´Ÿé¢æ¸…å•æ£€æŸ¥<br/>check.py"]
        GET_RULES["ä»æ•°æ®åº“è·å–<br/>100æ¡è´Ÿé¢æ¸…å•è§„åˆ™"]
        MATCH["è§„åˆ™åŒ¹é…<br/>å…³é”®è¯ + æ­£åˆ™"]
        GROUP["æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„"]
    end

    subgraph DATA2["ä¸­é—´æ•°æ® 2"]
        VIOLATIONS["violations<br/>[{clause_index, rule,<br/>description, severity,<br/>category, remediation}, ...]"]
        VIOL_SUM["summary<br/>{high: 2, medium: 5, low: 3}"]
    end

    subgraph STEP3["Step 3: å®šä»·åˆ†æ<br/>scoring.py"]
        ANAL_MORT["analyze_mortality()<br/>æ­»äº¡ç‡åˆ†æ"]
        ANAL_INT["analyze_interest()<br/>åˆ©ç‡åˆ†æ"]
        ANAL_EXP["analyze_expense()<br/>è´¹ç”¨ç‡åˆ†æ"]
        CALC_SCORE["calculate_overall_score()<br/>è®¡ç®—ç»¼åˆè¯„åˆ†"]
        GEN_REC["generate_recommendations()<br/>ç”Ÿæˆæ”¹è¿›å»ºè®®"]
    end

    subgraph DATA3["ä¸­é—´æ•°æ® 3"]
        PRICING["pricing<br/>{mortality: {value, benchmark,<br/>deviation, reasonable},<br/>interest: {...},<br/>expense: {...}}"]
        OVERALL["overall_score: 95<br/>is_reasonable: true<br/>recommendations: [...]"]
    end

    subgraph STEP4["Step 4: æŠ¥å‘Šç”Ÿæˆ<br/>report.py"]
        CALC_TOT["calculate_score()<br/>è®¡ç®—æ€»åˆ† 100-è¿è§„-å®šä»·"]
        CALC_GRADE["calculate_grade()<br/>è®¡ç®—è¯„çº§ ä¼˜ç§€/è‰¯å¥½/åˆæ ¼/ä¸åˆæ ¼"]
        GEN_SUM["generate_summary()<br/>ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡"]
        GEN_BASIS["generate_regulation_basis()<br/>åŠ¨æ€ç”Ÿæˆå®¡æ ¸ä¾æ®"]
        GEN_CONC["generate_conclusion_text()<br/>ç”Ÿæˆå®¡æ ¸ç»“è®º"]
        GEN_RPT["generate_report_content()<br/>ç”ŸæˆMarkdownæŠ¥å‘Š"]
        GEN_BLK["create_report()<br/>ç”ŸæˆæŠ¥å‘Šå—"]
    end

    subgraph DATA4["ä¸­é—´æ•°æ® 4"]
        SCORE["score: 75<br/>grade: 'åˆæ ¼'"]
        RPT_SUM["summary<br/>{total_violations: 10,<br/>violation_severity: {...},<br/>pricing_issues: 1,<br/>has_issues: true}"]
        REG_BASIS["regulation_basis<br/>['ä¿é™©æ³•', 'å¥åº·é™©ç®¡ç†åŠæ³•', ...]"]
        CONTENT["report_content<br/>Markdownæ ¼å¼æŠ¥å‘Š"]
        BLOCKS["blocks<br/>é£ä¹¦æ–‡æ¡£å—æ•°ç»„"]
    end

    subgraph STEP5["Step 5: é£ä¹¦å¯¼å‡º"]
        GET_TOKEN["get_feishu_access_token()<br/>è·å–è®¿é—®ä»¤ç‰Œ"]
        CREATE_DOC["create_feishu_document()<br/>åˆ›å»ºæ–‡æ¡£"]
        WRITE_BLK["æ‰¹é‡å†™å…¥å—<br/>æ¯æ¬¡50ä¸ª"]
    end

    subgraph OUTPUT["æœ€ç»ˆè¾“å‡º"]
        DOC_URL["æ–‡æ¡£URL<br/>https://feishu.cn/docx/xxx"]
        RPT_ID["æŠ¥å‘Šç¼–å·<br/>RPT-20260217-143020"]
        RESULT["å®Œæ•´ç»“æœ<br/>JSONæ ¼å¼"]
    end

    %% è¿æ¥å…³ç³»
    DOC_IN --> PARSE1
    URL_IN --> EXT_PROD
    TYPE_IN --> STEP3

    PARSE1 --> EXT_PROD
    EXT_PROD --> PROD_INFO
    PARSE1 --> EXT_CLAUSE
    EXT_CLAUSE --> CLAUSES
    PARSE1 --> EXT_PRICE
    EXT_PRICE --> PRICE_PARAMS

    CLAUSES --> STEP2
    GET_RULES --> MATCH
    MATCH --> GROUP
    GROUP --> VIOLATIONS
    GROUP --> VIOL_SUM

    PRICE_PARAMS --> STEP3
    PROD_INFO --> STEP4
    VIOLATIONS --> STEP4

    PRICE_PARAMS --> ANAL_MORT
    ANAL_MORT --> PRICING
    PRICE_PARAMS --> ANAL_INT
    ANAL_INT --> PRICING
    PRICE_PARAMS --> ANAL_EXP
    ANAL_EXP --> PRICING
    PRICING --> CALC_SCORE
    CALC_SCORE --> OVERALL
    OVERALL --> GEN_REC
    GEN_REC --> OVERALL

    VIOLATIONS --> CALC_TOT
    PRICING --> CALC_TOT
    CALC_TOT --> SCORE
    SCORE --> CALC_GRADE
    VIOLATIONS --> GEN_SUM
    PRICING --> GEN_SUM
    GEN_SUM --> RPT_SUM

    SCORE --> GEN_RPT
    GRADE --> GEN_RPT
    RPT_SUM --> GEN_RPT
    PROD_INFO --> GEN_RPT
    VIOLATIONS --> GEN_RPT
    PRICING --> GEN_RPT
    GEN_RPT --> CONTENT

    SCORE --> GEN_BLK
    GRADE --> GEN_BLK
    RPT_SUM --> GEN_BLK
    PROD_INFO --> GEN_BLK
    VIOLATIONS --> GEN_BLK
    PRICING --> GEN_BLK
    GEN_BLK --> BLOCKS

    BLOCKS --> STEP5
    GET_TOKEN --> CREATE_DOC
    CREATE_DOC --> WRITE_BLK
    WRITE_BLK --> DOC_URL

    CALC_TOT --> RESULT
    GEN_BLK --> RESULT
    RESULT --> RPT_ID

    %% æ ·å¼
    style INPUT fill:#e3f2fd
    style DATA1 fill:#fff3e0
    style DATA2 fill:#fff3e0
    style DATA3 fill:#fff3e0
    style DATA4 fill:#fff3e0
    style OUTPUT fill:#e8f5e9
    style STEP1 fill:#f3e5f5
    style STEP2 fill:#f3e5f5
    style STEP3 fill:#f3e5f5
    style STEP4 fill:#f3e5f5
    style STEP5 fill:#f3e5f5

    classDef dataBox fill:#fff3e0,stroke:#ff9800,stroke-width:2px
    class PROD_INFO,CLAUSES,PRICE_PARAMS,VIOLATIONS,VIOL_SUM,PRICING,OVERALL,SCORE,RPT_SUM,CONTENT,BLOCKS dataBox

    classDef stepBox fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px
    class PARSE1,EXT_PROD,EXT_CLAUSE,EXT_PRICE,GET_RULES,MATCH,GROUP,ANAL_MORT,ANAL_INT,ANAL_EXP,CALC_SCORE,GEN_REC,CALC_TOT,CALC_GRADE,GEN_SUM,GEN_BASIS,GEN_CONC,GEN_RPT,GEN_BLK,GET_TOKEN,CREATE_DOC,WRITE_BLK stepBox
```

### 8.2 æ•°æ®ç»“æ„è¯¦è§£

#### 8.2.1 äº§å“ä¿¡æ¯æ•°æ®ç»“æ„ (product_info)

```json
{
  "product_name": "XXç»ˆèº«å¯¿é™©",
  "insurance_company": "XXäººå¯¿ä¿é™©è‚¡ä»½æœ‰é™å…¬å¸",
  "product_type": "å¯¿é™©",
  "insurance_period": "ç»ˆèº«",
  "payment_method": "å¹´äº¤",
  "age_range": "å‡ºç”Ÿæ»¡28å¤©è‡³65å‘¨å²",
  "occupation_class": "1-6ç±»"
}
```

#### 8.2.2 è¿è§„è®°å½•æ•°æ®ç»“æ„ (violations)

```json
[
  {
    "clause_index": 5,
    "clause_text": "ç¬¬6æ¡ æœ¬äº§å“ä¿è¯å¹´åŒ–æ”¶ç›Šç‡...",
    "rule": "N001",
    "description": "æ¡æ¬¾ä¸­åŒ…å«'ä¿è¯æ”¶ç›Š'å­—æ ·ï¼Œè¿åç›‘ç®¡è§„å®š",
    "severity": "high",
    "category": "é”€å”®è¯¯å¯¼",
    "remediation": "åˆ é™¤'ä¿è¯æ”¶ç›Š'ç›¸å…³è¡¨è¿°ï¼Œæ”¹ä¸º'æ¼”ç¤ºæ”¶ç›Š'"
  }
]
```

#### 8.2.3 å®šä»·åˆ†ææ•°æ®ç»“æ„ (pricing)

```json
{
  "mortality": {
    "value": 0.0005,
    "benchmark": 0.0005,
    "deviation": 0.0,
    "reasonable": true,
    "note": "æ­»äº¡ç‡/å‘ç”Ÿç‡ç¬¦åˆè¡Œä¸šæ ‡å‡†"
  },
  "interest": {
    "value": 0.035,
    "benchmark": 0.035,
    "deviation": 0.0,
    "reasonable": true,
    "note": "é¢„å®šåˆ©ç‡ç¬¦åˆç›‘ç®¡è§„å®š"
  },
  "expense": {
    "value": 0.12,
    "benchmark": 0.12,
    "deviation": 0.0,
    "reasonable": true,
    "note": "è´¹ç”¨ç‡ç¬¦åˆç›‘ç®¡è§„å®š"
  }
}
```

#### 8.2.4 è¯„åˆ†è®¡ç®—é€»è¾‘

```python
# åŸºç¡€åˆ† 100
score = 100

# è¿è§„æ‰£åˆ†
for violation in violations:
    if severity == 'high':    score -= 20
    elif severity == 'medium': score -= 10
    elif severity == 'low':    score -= 5

# å®šä»·é—®é¢˜æ‰£åˆ†
for category in ['mortality', 'interest', 'expense']:
    if not pricing[category]['reasonable']:
        score -= 10

# æœ€ç»ˆåˆ†æ•°èŒƒå›´ [0, 100]
```

#### 8.2.5 æŠ¥å‘Šæ‘˜è¦æ•°æ®ç»“æ„ (summary)

```json
{
  "total_violations": 10,
  "violation_severity": {
    "high": 2,
    "medium": 5,
    "low": 3
  },
  "pricing_issues": 1,
  "has_critical_issues": true
}
```

### 8.3 é£ä¹¦å—æ•°æ®ç»“æ„

```json
[
  {
    "block_type": 2,
    "text": {
      "elements": [{
        "text_run": {
          "content": "ä¿é™©äº§å“ç²¾ç®—å®¡æ ¸æŠ¥å‘Š",
          "style": {
            "bold": true,
            "text_size": "largest"
          }
        }
      }]
    }
  },
  {
    "block_type": 2,
    "text": {
      "elements": [{
        "text_run": {
          "content": "è¡¨1-1ï¼šå…³é”®æŒ‡æ ‡æ±‡æ€»è¡¨",
          "style": {
            "bold": true
          }
        }
      }]
    }
  },
  {
    "block_type": 2,
    "text": {
      "elements": [{
        "text_run": {
          "content": "åºå· | æŒ‡æ ‡é¡¹ | ç»“æœ | è¯´æ˜",
          "style": {
            "bold": true,
            "font_family": "Courier New"
          }
        }
      }]
    }
  }
]
```

---

## ä¹ã€æŠ¥å‘Šç”Ÿæˆ

### 9.1 æŠ¥å‘Šç”ŸæˆåŸåˆ™

| åŸåˆ™ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| **åŠ¨æ€ç”Ÿæˆ** | æ‰€æœ‰å†…å®¹åŸºäºå®é™…å®¡æ ¸ç»“æœ | æ— è¿è§„åˆ™ä¸æ˜¾ç¤ºè¿è§„è¡¨æ ¼ |
| **ç»“è®ºå…ˆè¡Œ** | å®¡æ ¸ç»“è®ºæ”¾åœ¨æœ€å‰é¢ | ä¸€ã€å®¡æ ¸ç»“è®º |
| **é—®é¢˜å¯¼å‘** | æœ‰é—®é¢˜æ‰å±•ç¤ºå¯¹åº”ç« èŠ‚ | æ— å®šä»·é—®é¢˜åˆ™çœç•¥å®šä»·åˆ†æ |
| **ä¾æ®æ˜ç¡®** | æ¯ä¸ªé—®é¢˜éƒ½æœ‰æ³•è§„ä¾æ® | è¿è§„æè¿°+æ³•è§„æ¡æ¬¾+æ•´æ”¹å»ºè®® |

### 9.2 æŠ¥å‘Šç»“æ„ï¼ˆåŠ¨æ€ï¼‰

```
ä¿é™©äº§å“ç²¾ç®—å®¡æ ¸æŠ¥å‘Š
â”œâ”€â”€ äº§å“åŸºæœ¬ä¿¡æ¯
â”‚   â”œâ”€â”€ äº§å“åç§°ã€ä¿é™©å…¬å¸
â”‚   â”œâ”€â”€ äº§å“ç±»å‹ã€å®¡æ ¸æ—¥æœŸ
â”‚   â””â”€â”€ æŠ¥å‘Šç¼–å·
â”‚
â”œâ”€â”€ ä¸€ã€å®¡æ ¸ç»“è®ºï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼‰
â”‚   â”œâ”€â”€ å®¡æ ¸æ„è§ï¼ˆä¸æ¨è/æ¡ä»¶æ¨è/éœ€è¡¥å……ææ–™/æ¨èï¼‰
â”‚   â”œâ”€â”€ æ ¸å¿ƒé—®é¢˜æ‘˜è¦ï¼ˆ1-2å¥è¯ï¼‰
â”‚   â””â”€â”€ å…³é”®æŒ‡æ ‡æ±‡æ€»è¡¨
â”‚       â”œâ”€â”€ ç»¼åˆè¯„åˆ†
â”‚       â”œâ”€â”€ åˆè§„è¯„çº§
â”‚       â”œâ”€â”€ è¿è§„æ€»æ•°
â”‚       â””â”€â”€ å®šä»·è¯„ä¼°
â”‚
â”œâ”€â”€ äºŒã€é—®é¢˜è¯¦æƒ…åŠä¾æ®ï¼ˆæœ‰é—®é¢˜æ—¶æ˜¾ç¤ºï¼‰
â”‚   â”œâ”€â”€ å®¡æ ¸ä¾æ®ï¼ˆåŠ¨æ€ç”Ÿæˆï¼‰
â”‚   â”‚   â”œâ”€â”€ åŸºç¡€æ³•è§„ï¼ˆä¿é™©æ³•ç­‰ï¼‰
â”‚   â”‚   â”œâ”€â”€ äº§å“ç±»å‹ä¸“é¡¹æ³•è§„ï¼ˆå¥åº·é™©ç®¡ç†åŠæ³•ç­‰ï¼‰
â”‚   â”‚   â””â”€â”€ è¿è§„ç›¸å…³æ³•è§„å¼•ç”¨
â”‚   â”œâ”€â”€ è¿è§„ç»Ÿè®¡è¡¨
â”‚   â”œâ”€â”€ ä¸¥é‡è¿è§„æ˜ç»†ï¼ˆå¦‚æœ‰ï¼‰
â”‚   â”œâ”€â”€ ä¸­ç­‰è¿è§„æ˜ç»†ï¼ˆå¦‚æœ‰ï¼‰
â”‚   â””â”€â”€ å®šä»·é—®é¢˜åˆ†æï¼ˆå¦‚æœ‰ï¼‰
â”‚
â”œâ”€â”€ ä¸‰ã€ä¿®æ”¹å»ºè®®ï¼ˆæœ‰é—®é¢˜æ—¶æ˜¾ç¤ºï¼‰
â”‚   â”œâ”€â”€ P0çº§æ•´æ”¹äº‹é¡¹ï¼ˆå¦‚æœ‰ä¸¥é‡è¿è§„ï¼‰
â”‚   â””â”€â”€ P1çº§æ•´æ”¹äº‹é¡¹ï¼ˆå¦‚æœ‰ä¸­ç­‰è¿è§„ï¼‰
â”‚
â””â”€â”€ å››ã€æŠ¥å‘Šä¿¡æ¯ï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼‰
    â”œâ”€â”€ æŠ¥å‘Šç¼–å·/ç”Ÿæˆæ—¶é—´/ç³»ç»Ÿç‰ˆæœ¬
    â””â”€â”€ å…è´£å£°æ˜
```

### 9.3 æŠ¥å‘Šç”Ÿæˆå‡½æ•°æ¶æ„

```mermaid
flowchart TB
    subgraph REPORT["æŠ¥å‘Šç”Ÿæˆæ¨¡å— report.py"]
        EXECUTE["execute()<br/>ä¸»å…¥å£å‡½æ•°"]

        subgraph CALC["è®¡ç®—å±‚"]
            CALC_SCORE["calculate_score()<br/>è®¡ç®—æ€»åˆ†"]
            CALC_GRADE["calculate_grade()<br/>è®¡ç®—è¯„çº§"]
            GEN_SUM["generate_summary()<br/>ç”Ÿæˆæ‘˜è¦"]
        end

        subgraph GEN["ç”Ÿæˆå±‚"]
            GEN_BASIS["generate_regulation_basis()<br/>åŠ¨æ€ç”Ÿæˆå®¡æ ¸ä¾æ®"]
            GEN_CONC["generate_conclusion_text()<br/>ç”Ÿæˆå®¡æ ¸ç»“è®º"]

            subgraph MD["Markdownç”Ÿæˆ"]
                GEN_CONC_SEC["_generate_conclusion_section()<br/>ç»“è®ºç« èŠ‚"]
                GEN_DET_SEC["_generate_details_section()<br/>è¯¦æƒ…ç« èŠ‚"]
                GEN_SUG_SEC["_generate_suggestions_section()<br/>å»ºè®®ç« èŠ‚"]
                GEN_INFO_SEC["_generate_info_section()<br/>ä¿¡æ¯ç« èŠ‚"]
            end

            subgraph BLOCKS["é£ä¹¦å—ç”Ÿæˆ"]
                CREATE_CONC["_create_conclusion_blocks()<br/>ç»“è®ºå—"]
                CREATE_DET["_create_details_blocks()<br/>è¯¦æƒ…å—"]
                CREATE_SUG["_create_suggestions_blocks()<br/>å»ºè®®å—"]
                CREATE_INFO["_create_info_blocks()<br/>ä¿¡æ¯å—"]
            end
        end

        subgraph OUTPUT["è¾“å‡ºå±‚"]
            GEN_RPT["generate_report_content()<br/>MarkdownæŠ¥å‘Š"]
            CREATE_RPT["create_report()<br/>é£ä¹¦å—"]
        end
    end

    EXECUTE --> CALC
    CALC --> GEN
    GEN_BASIS --> GEN_DET_SEC
    GEN_BASIS --> CREATE_DET
    GEN_CONC --> GEN_CONC_SEC
    GEN_CONC --> CREATE_CONC

    GEN_CONC_SEC --> GEN_RPT
    GEN_DET_SEC --> GEN_RPT
    GEN_SUG_SEC --> GEN_RPT
    GEN_INFO_SEC --> GEN_RPT

    CREATE_CONC --> CREATE_RPT
    CREATE_DET --> CREATE_RPT
    CREATE_SUG --> CREATE_RPT
    CREATE_INFO --> CREATE_RPT

    style REPORT fill:#e3f2fd
    style CALC fill:#fff3e0
    style GEN fill:#f3e5f5
    style OUTPUT fill:#e8f5e9
    style GEN_BASIS fill:#ffcc80
    style GEN_CONC fill:#ffcc80
```

### 9.4 å®¡æ ¸ä¾æ®åŠ¨æ€ç”Ÿæˆ

```python
def generate_regulation_basis(violations, product_info):
    """
    åŠ¨æ€ç”Ÿæˆå®¡æ ¸ä¾æ®

    åŸºäºäº§å“ç±»å‹å’Œè¿è§„æƒ…å†µï¼ŒåŠ¨æ€ç”Ÿæˆé€‚ç”¨çš„æ³•è§„ä¾æ®åˆ—è¡¨
    """
    basis = []

    # 1. åŸºç¡€æ³•è§„ï¼ˆå§‹ç»ˆé€‚ç”¨ï¼‰
    basis.append("ã€Šä¸­åäººæ°‘å…±å’Œå›½ä¿é™©æ³•ã€‹")

    # 2. æ ¹æ®äº§å“ç±»å‹æ·»åŠ ä¸“é¡¹æ³•è§„
    type_regulations = {
        'å¯¿é™©': 'ã€Šäººèº«ä¿é™©å…¬å¸ä¿é™©æ¡æ¬¾å’Œä¿é™©è´¹ç‡ç®¡ç†åŠæ³•ã€‹',
        'å¥åº·é™©': 'ã€Šå¥åº·ä¿é™©ç®¡ç†åŠæ³•ã€‹',
        'æ„å¤–é™©': 'ã€Šæ„å¤–ä¼¤å®³ä¿é™©ç®¡ç†åŠæ³•ã€‹',
        'ä¸‡èƒ½é™©': 'ã€Šä¸‡èƒ½å‹äººèº«ä¿é™©ç®¡ç†åŠæ³•ã€‹',
    }

    product_type = product_info.get('product_type', '').lower()
    for key, regulation in type_regulations.items():
        if key in product_type:
            basis.append(regulation)
            break

    # 3. æå–è¿è§„è®°å½•ä¸­å¼•ç”¨çš„æ³•è§„
    for v in violations:
        if v.get('regulation_citation'):
            basis.append(v['regulation_citation'])

    return basis
```

### 9.5 å®¡æ ¸ç»“è®ºç”Ÿæˆ

```python
def generate_conclusion_text(score, summary):
    """
    ç”Ÿæˆå®¡æ ¸ç»“è®ºæ–‡æœ¬

    åŸºäºè¯„åˆ†å’Œè¿è§„æƒ…å†µï¼ŒåŠ¨æ€ç”Ÿæˆå®¡æ ¸æ„è§å’Œè¯´æ˜
    """
    high_count = summary['violation_severity']['high']

    if high_count > 0:
        return "ä¸æ¨èä¸Šä¼š", f"å­˜åœ¨{high_count}é¡¹ä¸¥é‡è¿è§„ï¼Œè§¦åŠç›‘ç®¡çº¢çº¿"
    elif score >= 90:
        return "æ¨èé€šè¿‡", "äº§å“ç¬¦åˆæ‰€æœ‰ç›‘ç®¡è¦æ±‚"
    elif score >= 75:
        return "æ¡ä»¶æ¨è", "å­˜åœ¨ä¸­ç­‰é—®é¢˜ï¼Œå»ºè®®æ•´æ”¹åæäº¤"
    elif score >= 60:
        return "éœ€è¡¥å……ææ–™", "å­˜åœ¨é—®é¢˜ï¼Œéœ€è¡¥å……è¯´æ˜ææ–™"
    else:
        return "ä¸äºˆæ¨è", "äº§å“åˆè§„æ€§ä¸è¶³"
```

### 9.6 æ¡ä»¶æ¸²æŸ“é€»è¾‘

```python
def generate_report_content(violations, pricing_analysis, product_info, score, grade, summary):
    """
    åŠ¨æ€ç”ŸæˆæŠ¥å‘Šå†…å®¹

    åªåœ¨æœ‰æ•°æ®æ—¶æ˜¾ç¤ºå¯¹åº”ç« èŠ‚
    """
    lines = []

    # å®¡æ ¸ç»“è®ºï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼‰
    lines.extend(_generate_conclusion_section(score, grade, summary))

    # é—®é¢˜è¯¦æƒ…ï¼ˆæœ‰é—®é¢˜æ—¶æ˜¾ç¤ºï¼‰
    if summary.get('has_issues', False):
        lines.extend(_generate_details_section(violations, pricing_analysis, product_info, summary))

    # ä¿®æ”¹å»ºè®®ï¼ˆæœ‰é—®é¢˜æ—¶æ˜¾ç¤ºï¼‰
    if summary.get('has_issues', False):
        lines.extend(_generate_suggestions_section(violations, summary))

    # æŠ¥å‘Šä¿¡æ¯ï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼‰
    lines.extend(_generate_info_section(report_id))

    return '\n'.join(lines)
```

### 9.7 æŠ¥å‘Šæ¨¡æ¿ç¤ºä¾‹

#### æ— é—®é¢˜æŠ¥å‘Šæ¨¡æ¿

```markdown
# ä¿é™©äº§å“ç²¾ç®—å®¡æ ¸æŠ¥å‘Š

äº§å“åç§°ï¼šXXç»ˆèº«å¯¿é™©
ä¿é™©å…¬å¸ï¼šXXäººå¯¿ä¿é™©è‚¡ä»½æœ‰é™å…¬å¸
å®¡æ ¸æ—¥æœŸï¼š2026å¹´02æœˆ17æ—¥
æŠ¥å‘Šç¼–å·ï¼šRPT-20260217-143020

## ä¸€ã€å®¡æ ¸ç»“è®º

**å®¡æ ¸æ„è§**ï¼šæ¨èé€šè¿‡

**è¯´æ˜**ï¼šäº§å“ç¬¦åˆæ‰€æœ‰ç›‘ç®¡è¦æ±‚ï¼Œæœªå‘ç°è¿è§„é—®é¢˜ã€‚

| æŒ‡æ ‡é¡¹ | ç»“æœ | è¯´æ˜ |
|--------|------|------|
| ç»¼åˆè¯„åˆ† | 95åˆ† | äº§å“ä¼˜ç§€ï¼Œå»ºè®®å¿«é€Ÿé€šè¿‡ |
| åˆè§„è¯„çº§ | ä¼˜ç§€ | åŸºäºè¿è§„æ•°é‡å’Œä¸¥é‡ç¨‹åº¦è¯„å®š |
| è¿è§„æ€»æ•° | 0é¡¹ | æ— è¿è§„ |
| å®šä»·è¯„ä¼° | åˆç† | 0é¡¹å®šä»·å‚æ•°éœ€å…³æ³¨ |

## å››ã€æŠ¥å‘Šä¿¡æ¯

æŠ¥å‘Šç¼–å·ï¼šRPT-20260217-143020
ç”Ÿæˆæ—¶é—´ï¼š2026å¹´02æœˆ17æ—¥ 14:30
å®¡æ ¸ç³»ç»Ÿï¼šActuary Sleuth v3.0

å…è´£å£°æ˜ï¼šæœ¬æŠ¥å‘Šç”±AIç²¾ç®—å®¡æ ¸ç³»ç»Ÿç”Ÿæˆï¼Œä»…ä¾›å†…éƒ¨å‚è€ƒ...
```

#### æœ‰é—®é¢˜æŠ¥å‘Šæ¨¡æ¿

```markdown
## ä¸€ã€å®¡æ ¸ç»“è®º

**å®¡æ ¸æ„è§**ï¼šä¸æ¨èä¸Šä¼š

**è¯´æ˜**ï¼šå­˜åœ¨2é¡¹ä¸¥é‡è¿è§„ï¼Œè§¦åŠç›‘ç®¡çº¢çº¿ï¼Œéœ€å®Œæˆæ•´æ”¹åé‡æ–°å®¡æ ¸ã€‚

## äºŒã€é—®é¢˜è¯¦æƒ…åŠä¾æ®

**å®¡æ ¸ä¾æ®**ï¼š
1. ã€Šä¸­åäººæ°‘å…±å’Œå›½ä¿é™©æ³•ã€‹
2. ã€Šå¥åº·ä¿é™©ç®¡ç†åŠæ³•ã€‹
3. ä¿é™©æ³•ç¬¬åä¸ƒæ¡ï¼ˆå…è´£æ¡æ¬¾ï¼‰

### 2.1 è¿è§„ç»Ÿè®¡

| çº§åˆ« | æ•°é‡ | å æ¯” |
|------|------|------|
| ä¸¥é‡ | 2é¡¹ | 20% |
| ä¸­ç­‰ | 5é¡¹ | 50% |

### 2.2 ä¸¥é‡è¿è§„æ˜ç»†

| è§„åˆ™ | è¿è§„æè¿° | æ¶‰åŠæ¡æ¬¾ | æ³•è§„ä¾æ® | æ•´æ”¹å»ºè®® |
|------|----------|----------|----------|----------|
| N001 | åŒ…å«ä¿è¯æ”¶ç›Šè¡¨è¿° | ç¬¬6æ¡ | ä¿é™©æ³•ç¬¬åä¸ƒæ¡ | æ”¹ä¸ºæ¼”ç¤ºæ”¶ç›Š |

## ä¸‰ã€ä¿®æ”¹å»ºè®®

### 3.1 P0çº§æ•´æ”¹äº‹é¡¹ï¼ˆå¿…é¡»ç«‹å³æ•´æ”¹ï¼‰

1. åˆ é™¤ç¬¬6æ¡ä¸­"ä¿è¯æ”¶ç›Š"ç›¸å…³è¡¨è¿°
2. è¡¥å……ç¬¬15æ¡çŠ¹è±«æœŸèµ·ç®—æ—¥æœŸ

## å››ã€æŠ¥å‘Šä¿¡æ¯
...
```

---

## åã€æ ¸å¿ƒæµç¨‹

### 10.1 å®¡æ ¸æµç¨‹

```python
def execute_audit(input_data):
    """æ‰§è¡Œå®Œæ•´å®¡æ ¸æµç¨‹"""

    # 1. æ–‡æ¡£é¢„å¤„ç†
    doc = preprocess.process(input_data['document_content'])
    # è¾“å‡º: {clauses, sections, pricing_data, ...}

    # 2. è´Ÿé¢æ¸…å•æ£€æŸ¥
    violations = check.negative_list(doc['clauses'])
    # è¾“å‡º: [{rule, description, severity, remediation}, ...]

    # 3. æ³•è§„åˆè§„æ£€æŸ¥
    for v in violations:
        v['regulations'] = query.search_regulations(v['description'])

    # 4. å®šä»·åˆ†æ
    pricing = scoring.analyze_pricing(doc['pricing_data'])
    # è¾“å‡º: {mortality, interest, expense, reasonableness}

    # 5. è®¡ç®—è¯„åˆ†
    score = scoring.calculate_score(violations, pricing)
    # è¾“å‡º: 0-100 åˆ†

    # 6. ç”ŸæˆæŠ¥å‘Š
    return report.generate({
        'violations': violations,
        'pricing': pricing,
        'score': score,
        'document': doc
    })
```

### 10.2 æŸ¥è¯¢æµç¨‹

```python
def search_regulation(query, search_type='hybrid'):
    """æ‰§è¡Œæ³•è§„æŸ¥è¯¢"""

    results = []

    # ç²¾ç¡®æŸ¥è¯¢
    if search_type in ['exact', 'hybrid']:
        exact = db.find_regulation(query)
        if exact:
            results.append({...})

    # è¯­ä¹‰æ£€ç´¢
    if search_type in ['semantic', 'hybrid']:
        query_vec = ollama.embed(query)
        semantic = lancedb.search(query_vec, top_k=5)
        results.extend(semantic)

    # æ’åºè¿”å›
    results.sort(key=lambda x: x['score'], reverse=True)
    return results[:5]
```

---

## åä¸€ã€åˆå§‹åŒ–è„šæœ¬

### 11.1 scripts/init_db.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆå§‹åŒ–æ•°æ®åº“
"""
import sqlite3
from pathlib import Path

DB_PATH = Path(__file__).parent.parent / 'data' / 'actuary.db'

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
    conn = sqlite3.connect(DB_PATH)

    # åˆ›å»ºæ³•è§„è¡¨
    conn.execute('''
        CREATE TABLE IF NOT EXISTS regulations (
            id TEXT PRIMARY KEY,
            law_name TEXT NOT NULL,
            article_number TEXT,
            content TEXT NOT NULL,
            category TEXT,
            tags TEXT,
            effective_date TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    conn.execute('''
        CREATE INDEX IF NOT EXISTS idx_article
        ON regulations(law_name, article_number)
    ''')

    # åˆ›å»ºè´Ÿé¢æ¸…å•è¡¨
    conn.execute('''
        CREATE TABLE IF NOT EXISTS negative_list (
            id INTEGER PRIMARY KEY,
            rule_number TEXT UNIQUE,
            description TEXT NOT NULL,
            severity TEXT,
            category TEXT,
            remediation TEXT,
            keywords TEXT,
            patterns TEXT,
            version TEXT,
            effective_date TEXT
        )
    ''')

    # åˆ›å»ºå®¡æ ¸å†å²è¡¨
    conn.execute('''
        CREATE TABLE IF NOT EXISTS audit_history (
            id TEXT PRIMARY KEY,
            user_id TEXT,
            document_url TEXT,
            document_type TEXT,
            violations TEXT,
            score REAL,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')

    conn.commit()
    conn.close()
    print(f"Database initialized: {DB_PATH}")

if __name__ == '__main__':
    init_database()
```

### 11.2 scripts/import_regs.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¼å…¥æ³•è§„æ•°æ®
"""
import sqlite3
import re
from pathlib import Path

DB_PATH = Path(__file__).parent.parent / 'data' / 'actuary.db'
REFS_PATH = Path(__file__).parent.parent / 'references'

def import_markdown_file(file_path):
    """å¯¼å…¥å•ä¸ª Markdown æ³•è§„æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # è§£ææ³•è§„åç§°
    law_name = file_path.stem

    # è§£ææ¡æ¬¾
    articles = parse_articles(content)

    # å†™å…¥æ•°æ®åº“
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()

    for article in articles:
        cur.execute('''
            INSERT OR REPLACE INTO regulations
            (id, law_name, article_number, content, category)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            f"{law_name}_{article['number']}",
            law_name,
            article['number'],
            article['content'],
            article.get('category', '')
        ))

    conn.commit()
    conn.close()
    print(f"Imported {len(articles)} articles from {file_path.name}")

def parse_articles(content):
    """è§£æ Markdown æ–‡ä»¶ä¸­çš„æ¡æ¬¾"""
    articles = []

    # åŒ¹é…æ¡æ¬¾æ ‡é¢˜ï¼ˆå¦‚ï¼šç¬¬åå…­æ¡ã€16.ï¼‰
    pattern = r'^(#{1,3}\s*)?(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡|[\d]+\.?)\s*(.*)$'

    current_article = None

    for line in content.split('\n'):
        match = re.match(pattern, line.strip())
        if match:
            # ä¿å­˜ä¸Šä¸€æ¡
            if current_article:
                articles.append(current_article)

            # å¼€å§‹æ–°æ¡æ¬¾
            current_article = {
                'number': match.group(2),
                'title': match.group(3),
                'content': ''
            }
        elif current_article:
            # è¿½åŠ å†…å®¹
            current_article['content'] += line + '\n'

    # ä¿å­˜æœ€åä¸€æ¡
    if current_article:
        articles.append(current_article)

    return articles

def import_all_references():
    """å¯¼å…¥æ‰€æœ‰å‚è€ƒèµ„æ–™"""
    refs_path = REFS_PATH
    if not refs_path.exists():
        print(f"References directory not found: {refs_path}")
        return

    for md_file in refs_path.glob('*.md'):
        import_markdown_file(md_file)

if __name__ == '__main__':
    import_all_references()
```

### 11.3 scripts/build_vectors.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ„å»ºå‘é‡ç´¢å¼•
"""
import sqlite3
from pathlib import Path
from lib import lancedb, ollama

DB_PATH = Path(__file__).parent.parent / 'data' / 'actuary.db'

def build_vector_index():
    """æ„å»ºæ³•è§„å‘é‡ç´¢å¼•"""
    # è¯»å–æ‰€æœ‰æ³•è§„
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()

    cur.execute('SELECT id, law_name, article_number, content FROM regulations')
    regulations = cur.fetchall()
    conn.close()

    print(f"Building vectors for {len(regulations)} regulations...")

    vectors_data = []
    for reg_id, law_name, article_number, content in regulations:
        # ç”Ÿæˆåˆ†å—å‘é‡
        chunks = split_content(content)

        for idx, chunk in enumerate(chunks):
            # ç”Ÿæˆå‘é‡
            vector = ollama.embed(chunk)

            vectors_data.append({
                'id': f"{reg_id}_{idx}",
                'regulation_id': reg_id,
                'chunk_text': chunk,
                'vector': vector,
                'metadata': {
                    'law_name': law_name,
                    'article_number': article_number
                }
            })

        print(f"Processed: {law_name} - {article_number}")

    # å†™å…¥ LanceDB
    lancedb.add_vectors(vectors_data)
    print(f"Built {len(vectors_data)} vectors")

def split_content(content, max_length=500):
    """å°†å†…å®¹åˆ†å—"""
    chunks = []
    current_chunk = ""

    for paragraph in content.split('\n\n'):
        if len(current_chunk) + len(paragraph) > max_length:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = paragraph
        else:
            current_chunk += "\n\n" + paragraph if current_chunk else paragraph

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

if __name__ == '__main__':
    build_vector_index()
```

---

## åäºŒã€éƒ¨ç½²è¯´æ˜

### 12.1 ç¯å¢ƒè¦æ±‚

| ç»„ä»¶ | ç‰ˆæœ¬è¦æ±‚ |
|------|----------|
| Python | 3.10+ |
| Ollama | æœ€æ–°ç‰ˆ |
| SQLite | ç³»ç»Ÿè‡ªå¸¦ |

### 12.2 Python ä¾èµ–

```
# scripts/requirements.txt
lancedb>=0.5.0
requests>=2.28.0
pyarrow>=14.0.0
paddleocr>=2.7.0
feishu2md>=0.1.0
```

### 12.3 å®‰è£…æ­¥éª¤

```bash
# 1. è¿›å…¥ Skill ç›®å½•
cd /root/.openclaw/workspace/skills/actuary-sleuth

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ï¼‰
python3 -m venv venv
source venv/bin/activate

# 3. å®‰è£… Python ä¾èµ–
pip install -r scripts/requirements.txt

# 4. ç¡®è®¤ Ollama è¿è¡Œ
ollama list

# 5. åˆå§‹åŒ–æ•°æ®åº“
python3 scripts/init_db.py

# 6. å¯¼å…¥æ³•è§„æ•°æ®
python3 scripts/import_regs.py

# 7. æ„å»ºå‘é‡ç´¢å¼•
python3 scripts/build_vectors.py

# 8. æµ‹è¯•è„šæœ¬
python3 scripts/query.py '{"query":"ä¿é™©æ³•ç¬¬åå…­æ¡"}'

# 9. é‡æ–°åŠ è½½ Skill
openclaw skills reload
```

---

## åä¸‰ã€äº¤äº’ç¤ºä¾‹

### 13.1 å®¡æ ¸äº¤äº’

```
ç”¨æˆ·: @actuary å®¡æ ¸ https://xxx.feishu.cn/doc/xxxxx

æœºå™¨äºº: ğŸ” æ­£åœ¨å®¡æ ¸æ–‡æ¡£ï¼Œè¯·ç¨å€™...

[çº¦30ç§’å]

æœºå™¨äºº: ğŸ“Š å®¡æ ¸æŠ¥å‘Š

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ç»¼åˆè¯„åˆ†ã€‘75 åˆ† - åˆæ ¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”´ é«˜å±è¿è§„ (2)
  1. å…è´£æ¡æ¬¾æœªåŠ ç²—æ ‡çº¢
     - ä½ç½®: ç¬¬åäº”æ¡
     - æ³•è§„: ä¿é™©æ³•ç¬¬åä¸ƒæ¡
     - å»ºè®®: è¯·å°†å…è´£æ¡æ¬¾åŠ ç²—å¹¶ä½¿ç”¨çº¢è‰²å­—ä½“

  2. çŠ¹è±«æœŸæè¿°ä¸å®Œæ•´
     - ä½ç½®: ç¬¬äºŒåæ¡
     - æ³•è§„: äººèº«ä¿é™©ä¸šåŠ¡ç›‘ç®¡åŠæ³•ç¬¬Xæ¡
     - å»ºè®®: è¡¥å……çŠ¹è±«æœŸèµ·ç®—æ—¥æœŸ

ğŸŸ¡ ä¸­å±è¿è§„ (1)
  1. è´¹ç‡è¡¨å±•ç¤ºæ–¹å¼éœ€ä¼˜åŒ–
     - ä½ç½®: è´¹ç‡è¡¨
     - å»ºè®®: å»ºè®®å¢åŠ è´¹ç‡è¯´æ˜

âœ… å®šä»·åˆ†æåˆç†

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
å®¡æ ¸æ—¶é—´: 2026-02-15 14:30
å®¡æ ¸ç¼–å·: AUD-20260215-001
```

### 13.2 æŸ¥è¯¢äº¤äº’

```
ç”¨æˆ·: @actuary æŸ¥è¯¢ ä¿é™©æ³•ç¬¬åå…­æ¡

æœºå™¨äºº: ğŸ“– ä¿é™©æ³• ç¬¬åå…­æ¡

è®¢ç«‹ä¿é™©åˆåŒï¼Œä¿é™©äººå°±ä¿é™©æ ‡çš„æˆ–è€…è¢«ä¿é™©äººçš„æœ‰å…³æƒ…å†µæå‡ºè¯¢é—®çš„ï¼ŒæŠ•ä¿äººåº”å½“å¦‚å®å‘ŠçŸ¥ã€‚

æŠ•ä¿äººæ•…æ„æˆ–è€…å› é‡å¤§è¿‡å¤±æœªå±¥è¡Œå‰æ¬¾è§„å®šçš„å¦‚å®å‘ŠçŸ¥ä¹‰åŠ¡ï¼Œè¶³ä»¥å½±å“ä¿é™©äººå†³å®šæ˜¯å¦åŒæ„æ‰¿ä¿æˆ–è€…æé«˜ä¿é™©è´¹ç‡çš„ï¼Œä¿é™©äººæœ‰æƒè§£é™¤åˆåŒã€‚

æŠ•ä¿äººæ•…æ„ä¸å±¥è¡Œå¦‚å®å‘ŠçŸ¥ä¹‰åŠ¡çš„ï¼Œä¿é™©äººå¯¹äºåˆåŒè§£é™¤å‰å‘ç”Ÿçš„ä¿é™©äº‹æ•…ï¼Œä¸æ‰¿æ‹…èµ”å¿æˆ–è€…ç»™ä»˜ä¿é™©é‡‘çš„è´£ä»»ï¼Œå¹¶ä¸é€€è¿˜ä¿é™©è´¹ã€‚

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
åˆ†ç±»: å¦‚å®å‘ŠçŸ¥ä¹‰åŠ¡ | ç”Ÿæ•ˆæ—¥æœŸ: 2009-10-01
```

---

## åå››ã€æ€»ç»“

### 14.1 æ ¸å¿ƒç‰¹ç‚¹

1. **å·¥ä½œæµç¼–æ’ä¸å®ç°åˆ†ç¦»**: SKILL.md å®šä¹‰å·¥ä½œæµï¼ŒPython è„šæœ¬å®ç°åŠŸèƒ½
2. **æ— èƒ¶æ°´å±‚**: SKILL.md ç›´æ¥å£°æ˜è„šæœ¬ï¼Œè¢«è§£æåç›´æ¥è°ƒç”¨
3. **æœ¬åœ°åŒ–éƒ¨ç½²**: æ‰€æœ‰æ•°æ®å­˜å‚¨å’Œå¤„ç†åœ¨æœ¬åœ°
4. **æ ‡å‡†åŒ–æ¥å£**: ç»Ÿä¸€çš„ Python è„šæœ¬æ¥å£è§„èŒƒ
5. **å¯æ‰©å±•æ€§**: æ˜“äºæ·»åŠ æ–°çš„å®¡æ ¸è§„åˆ™å’Œæ³•è§„æ•°æ®

### 14.2 æŠ€æœ¯ä¼˜åŠ¿

| ä¼˜åŠ¿ | è¯´æ˜ |
|------|------|
| ç®€æ´ | æ— éœ€ JS ä»£ç ï¼ŒSKILL.md + Python è„šæœ¬å³å¯ |
| é«˜æ•ˆ | ç›´æ¥è„šæœ¬è°ƒç”¨ï¼Œæ— ä¸­é—´å±‚å¼€é”€ |
| å¯ç»´æŠ¤ | Python è„šæœ¬ç‹¬ç«‹ï¼Œä¾¿äºè°ƒè¯•å’Œæ‰©å±• |
| çµæ´» | æ”¯æŒæ··åˆæ£€ç´¢ï¼ˆç²¾ç¡®+è¯­ä¹‰ï¼‰ |
| æœ¬åœ°åŒ– | æ•°æ®ä¸å‡ºå†…ç½‘ï¼Œå®‰å…¨å¯æ§ |

### 14.3 åç»­ä¼˜åŒ–æ–¹å‘

1. å¢åŠ æ›´å¤šå®¡æ ¸è§„åˆ™å’Œæ³•è§„æ•°æ®
2. ä¼˜åŒ–å®šä»·åˆ†æç®—æ³•
3. æ”¯æŒæ›´å¤šæ–‡æ¡£æ ¼å¼
4. å¢åŠ å®¡æ ¸å†å²åˆ†æåŠŸèƒ½
5. æ”¯æŒæ‰¹é‡å®¡æ ¸
